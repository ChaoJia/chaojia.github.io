[{"body":"","link":"https://chaojia.github.io/","section":"","tags":null,"title":""},{"body":"As part of the effort to get myself familiar with Vulkan, I developed a real-time renderer featuring global illumination with RTX technique 1. Aside from Dynamic Diffuse Global Illumination (DDGI) 2, ray-traced soft shadow and specular reflections with spatial temporal-denoising (SVGF 3), I've also tried out some other interesting ideas.\nVisibility buffer rendering and scene management Visibility buffer rendering 4 has gained its popularity as triangle meshes used in real-time applications are getting much finer details thanks to the rapidly increasing memory capacity and computational power of the GPU 5. More details for triangle meshes generally means fewer pixels each triangle can cover. Due to low quad utilization 6, traditional deferred shading tends to get very inefficient since there is usually a lot of computation going on in the fragment shader to fill the G-buffer.\nSince there doesn't seem to be any solid reasons against it, I decided to go with visibility buffer rendering, i.e., a simple G-buffer consisting of only triangle indices and depth values. One problem needs to be addressed is how to recover the geometric data required for shading from a triangle index, including world position, normal, uv coordinates at each pixel. Moreover, We need the derivatives of uv coordinates to determine the LOD of the textures. The derivatives of the world position may be useful for e.g. constructing cotangent frame. To get the computation of partial derivatives right, I did some math and detailed the method for analytically computing partial derivatives in my last post. Another piece of information needed for shading is material data, such as textures and alpha cutoff value (for e.g. tree leaves). To simplify scene management and avoid frequently updating descriptor sets, I took a bindless approach 7. Although many mobile devices and integrated cards are not very friendly to this approach due to lack of support for Vulkan feature shaderSampledImageArrayNonUniformIndexing or VK_EXT_descriptor_indexing, or the supported limits such as maxPerStageDescriptorSampledImages are too small for typical test scenes like Sponza, GPUs that support RTX are usually very generous in this regard.\nPartly due to a variety of different 3D formats, scene management can get quite complicated. Although I've dealt with .ply and .obj files while working on my master project, neither of them can really fulfill the requirement of a renderer by today's standard, at least in terms of materials that support Physically-Based Rendering (PBR). Therefore, this renderer only supports glTF format as it is royalty-free, extensible and has support for basic PBR materials among other advantages. Many other formats can be easily converted to glTF using blender. Despite the fact that glTF can store most of its data in binary form, loading glTF models that contain many textures can still be very slow because decoding common image formats (e.g. .jpg or .png) is rather computationally expensive, but directly storing the decoded image data in a cache file is not ideal either due to storage constraints. Inspired by this article, I also opted for block compression to reduce the cache file size while accelerating scene loading. To facilitate batch upload of textures to the device (GPU), I packed all the textures alongside other scene data into a single binary cache file.\nPutting everything together, I came up with the following binary format for the scene cache file:\n1// metadata to determine if the cache is up-to-date 2// punctual lights 3// aabb lower and upper 4vertex_offsets : uint32_t[material_type_count + 1] 5index_offsets : uint32_t[material_type_count + 1] 6mtl_param_emissive_idx_offsets : uint32_t[material_type_count + 1] 7mtl_param_alpha_cutoff_offsets : uint32_t[material_type_count + 1] 8emissive_texture_count : uint32_t // at least one black texture 9texture_strides : uint32_t[material_type_count] 10texture_offsets : uint32_t[material_type_count + 1] // texture index offset for each mtl_type 11position_buffer 12normal_buffer 13uv_buffer 14index_buffer 15triangle_mtl_idx_buffer 16mtl_param_buffer; // array of scene_t::mtl_param_type[] 17sampler_count: uint16_t 18sampler_infos 19texture_descriptor_array 20// metadata to verify data integrity The mesh data and material data are sorted by material types. Generally different material types need different rendering pipelines. For example, MTL_TYPE_PRIMARY corresponds to the metallic-roughness material with alphaMode set to the default value OPAQUE, while MTL_TYPE_ALPHA_CUTOFF differs only by setting alphaMode to MASK. An instance of a material type refers to a specific set of textures and material parameters. { vertex | index | mtl_param* | texture }_offsets are offsets of different material types in *_buffer or texture_descriptor_array. All *Factor fields in the glTF materials are either converted to 1x1 textures or multiplied into the corresponding texture. Occlusion, metallic and roughness factors are merged into one texture as different color channels. Since emissive textures are relatively uncommon, a 1x1 black image shared by non-emissive material instances is put at the beginning of texture_descriptor_array, followed by other emissive textures. vkCmdBlitImage, C++ std::thread and parallel version of C++ standard algorithms library are used for faster generation of the cache data.\nThe cache file can greatly reduce the scene loading time. Here is the time it takes to load the scene Amazon Lumberyard Bistro (exterior) (glTF + textures: 898 MB; cache file: 1.05 GB) from a SSD:\nglTF 1st load glTF 2nd load cache 1st load cache 2nd load cache generation 35.554 s 21.874 s 2.239 s 1.334 s 23.468 s The operating system (Windows) probably caches those files in RAM after the 1st load, hence the 2nd load is faster than the 1st load. The difference can be much larger for HDD. Indeed, loading the cache from my HDD for the first time can take 10 seconds, but subsequent loading only takes about 2 seconds.\nDuring the development of scene management, I noticed something interesting about the performance of std::vector. To avoid surprising behavior, std::vector (with default allocator) forces initialization on allocation, with either default zero-initialization or user provided value. Although this is the expected behavior for many use cases, it can unnecessarily hurt performance if the requested amount of memory is huge and the entire purpose of the allocation is to fill it with fixed amount of data. std::array can be used if the size is known at compile time and reasonably small. Otherwise plain array managed by std::unique_ptr is a better option:\n1const uint32_t num_f32_texels = 5592405; // texture: 2048 x 2048 x rgb, with full mipmap chain 2std::vector\u0026lt;glm::vec3\u0026gt; f32_texels(num_f32_texels, glm::vec3{}); // took 15.401600 ms 3std::vector\u0026lt;glm::vec3\u0026gt; f32_texels(num_f32_texels); // took 9.379000 ms 4std::unique_ptr\u0026lt;glm::vec3[]\u0026gt; f32_texels(new glm::vec3[num_f32_texels]); // took 0.014100 ms Another caveat came to my notice is the propertyFlags of Vulkan memory. One common operation is to copy a chunk of data from host to device, therefore device memory is often allocated with the flag HOST_COHERENT. However, to copy data from device to host, HOST_CACHED needs to be specified instead of HOST_COHERENT. It can make a huge difference. On my laptop (RTX 2070 Mobile), it takes 61.931 ms to copy 21 MB from a HOST_VISIBLE, optionally HOST_COHERENT buffer to the host memory, but only 3.839 ms if the buffer is HOST_CACHED.\nDynamic diffuse global illumination (DDGI) The implementation of DDGI is fairly straightforward, given the comprehensive description in the original papers 2 8 9 and some optimization techniques explained in this article. One slightly irritating thing is the additional step (VkDispatch) for simply copying the border texels after updating the irradiance textures, because synchronization is needed right before updating the border texels. One trick I've tried out is to treat the vertices of the grid for each probe as the value stored in the irradiance textures. That means we need a 7 x 7 grid for a probe resolution of 8 x 8, which is slightly lower than it would haven been, but the result differs not very much. Nevertheless, I finally decided to copy the border texels. To avoid the additional kernel launch, I figured out another trick after some observation. First I wrote a shadertoy app to visualize the octahedral mapping more clearly. With the help of the app, I started to got a hang of the pattern of the copy operations.\nshadertoy app to visualize octahedral mapping\nFor simplicity, let's assume the probe resolution is 8 x 8 (square with four yellow filled cells as its four corners in the following image), then the padded probe resolution is 10 x 10. For greenish or reddish cells, the source cell and destination cell of each copy operation need to be filled with exactly the same color. Obviously every such cell (henceforth referred to as type 1 texel) in the 8 x 8 square is only copied once, either vertically or horizontally. It is slightly more complicated for the yellow filled cells (henceforth referred to as type 2 texel), as each of them is copy three times, vertically, horizontally and diagonally.\nto be continued...\nSource code on gitlab\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZander Majercik, Jean-Philippe Guertin, Derek Nowrouzezahrai, and Morgan McGuire, Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields, Journal of Computer Graphics Techniques (JCGT), vol. 8, no. 2, 1-30, 2019\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchied, Christoph, et al. Spatiotemporal variance-guided filtering: real-time reconstruction for path-traced global illumination. Proceedings of High Performance Graphics. 2017. 1-12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChristopher A. Burns and Warren A. Hunt, The Visibility Buffer: A Cache-Friendly Approach to Deferred Shading, Journal of Computer Graphics Techniques (JCGT), vol. 2, no. 2, 55-69, 2013\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNanite - A Deep Dive\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis article Visibility Buffer Rendering with Material Graphs provided thorough analysis and comprehensive performance test on forward shading, deferred shading and visibility buffer rendering\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee the section Bindless descriptor designs of this article\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZander Majercik, Adam Marrs, Josef Spjut, and Morgan McGuire, Scaling Probe-Based Real-Time Dynamic Global Illumination for Production, Journal of Computer Graphics Techniques (JCGT), vol. 10, no. 2, 1-29, 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMorgan McGuire, Mike Mara, Derek Nowrouzezahrai, and David Luebke. 2017. Real-time global illumination using precomputed light field probes. In Proceedings of the 21st ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D '17). Association for Computing Machinery, New York, NY, USA, Article 2, 1-11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/posts/22-08-22-vulkan-basic-renderer/","section":"posts","tags":["real-time rendering","vulkan","RTX","C++"],"title":"A Basic Vulkan Renderer"},{"body":"","link":"https://chaojia.github.io/tags/c++/","section":"tags","tags":null,"title":"C++"},{"body":"","link":"https://chaojia.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://chaojia.github.io/categories/graphics/","section":"categories","tags":null,"title":"graphics"},{"body":"","link":"https://chaojia.github.io/posts/","section":"posts","tags":null,"title":"Posts"},{"body":"","link":"https://chaojia.github.io/tags/real-time-rendering/","section":"tags","tags":null,"title":"real-time rendering"},{"body":"","link":"https://chaojia.github.io/categories/rendering/","section":"categories","tags":null,"title":"rendering"},{"body":"","link":"https://chaojia.github.io/tags/rtx/","section":"tags","tags":null,"title":"RTX"},{"body":"","link":"https://chaojia.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://chaojia.github.io/tags/vulkan/","section":"tags","tags":null,"title":"vulkan"},{"body":"Real-Time C++ Vulkan Renderer with RTX global illumination Showcase of my Vulkan renderer featuring Dynamic Diffuse Global Illumination (DDGI) 1 2, ray-traced soft shadow and specular reflections with spatial-temporal denoising (SVGF 3). The renderer is implemented with C++, Vulkan and GLSL.\nSource code: https://gitlab.com/chao-jia/spock\nReal-Time Unified Physics Simulation of Variable Sized Particles This video showcases my master project done in 2017. This project is based on the position-based unified dynamic framework presented by Macklin et al 4. In this project, we removed the restriction of fixed radius of the particles in the same scene entailed by the aforementioned unified framework to reduce memory footprint of the physics simulation while maintaining the real-time performance (60+ fps on RTX 1060 Mobile).\nI also implemented and optimized algorithms for solid voxelization and construction of signed distance field on the GPU using CUDA to accelerate scene initialization. The scenes are rendered with OpenGL 4. For efficient collision detection between differently sized particles and generation of density constraints for fluids, I implemented BVH (Bounding Volume Hierarchy) construction and traversal on the GPU.\nsource code: https://gitlab.com/chao-jia/pbd\nHeightfield Water Simulation and Rendering Heightfield-based real-time water simulation with C++, Qt 5 and OpenGL 4 as a freely chosen topic for the final assignment of a practicum course in 2016 summer semester. Inspired by WebGL Water (https://madebyevan.com/webgl-water/).\nSource code: https://gitlab.com/chao-jia/height_field_water\nOther projects See here for more projects I've worked on.\nZander Majercik, Adam Marrs, Josef Spjut, and Morgan McGuire, Scaling Probe-Based Real-Time Dynamic Global Illumination for Production, Journal of Computer Graphics Techniques (JCGT), vol. 10, no. 2, 1-29, 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZander Majercik, Jean-Philippe Guertin, Derek Nowrouzezahrai, and Morgan McGuire, Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields, Journal of Computer Graphics Techniques (JCGT), vol. 8, no. 2, 1-30, 2019\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchied, Christoph, et al. Spatiotemporal variance-guided filtering: real-time reconstruction for path-traced global illumination. Proceedings of High Performance Graphics. 2017. 1-12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMiles Macklin, Matthias Müller, Nuttapong Chentanez, and Tae-Yong Kim. 2014. Unified particle physics for real-time applications. ACM Trans. Graph. 33, 4, Article 153 (July 2014), 12 pages.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/portfolio/","section":"","tags":null,"title":"Portfolio"},{"body":"Hi, my name is Chao Jia (IPA: /tʃaʊ. dʒʌ/). I started my work at TU Wien in 2018 as a research assistant in the Rendering and Modeling group after I got my Master's degree in computer science from Karlsruhe Institute of Technology. I'm interested in real-time rendering (OpenGL 4 and Vulkan), high-performance C++, physically based simulation and general-purpose computing on GPU (mainly CUDA, a bit OpenCL). CMake is no stranger to me either, as I've been using it as the build system for my C++ projects and vcpkg as package manager. Although I haven't worked on large python projects, I've been writing python scripts to automate many tasks.\nProjects Some of the projects I've worked on can be found in my portfolio. Here are some other projects during my work at TU Wien:\nFor the paper View-Dependent Impostors for Architectural Shape Grammars 1, I extended a C++ template library for shape grammar evaluation and implemented level-of-detail mechanism for procedural geometry shape grammars; For the paper On Provisioning Procedural Geometry Workloads on Edge Architectures 2, I implemented an efficient multi-threaded web service for procedural geometry workloads using Boost Beast library in C++. Protocol buffers (Protobuf) was used for client-server communication. I have put a lot of efforts into making sure the program is platform-agnostic and made a docker container for the application to facilitate fast deployment on different edge devices; I have also implemented a GPU shape grammar evaluation system using CUDA targeting NVidia Jetson devices; For the paper Sabrina: Modeling and Visualization of Economy Data with Incremental Domain Knowledge 3, in collaboration with other research groups, I familiarized myself with Javascript, ReactJS and Postgres, and put together a web application for the visualization of Austria financial data. I've also written some Python scripts for data processing (Source code on gitlab). During my free time, I have contributed a few new ports and fixes to vcpkg as I was trying to get a deeper understanding of CMake and vcpkg. I've also developed a blender addon to visualize .ply files containing point clouds with some custom attributes using Python (Source code on gitlab).\nPublications Chao Jia, Moritz Roth, Bernhard Kerbl, Michael Wimmer. View-Dependent Impostors for Architectural Shape Grammars. In Pacific Graphics Short Papers, Posters, and Work-in-Progress Papers, pages 63-64. October 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIlir Murturi, Chao Jia, Bernhard Kerbl, Michael Wimmer, Schahram Dustdar, Christos Tsigkanos. On Provisioning Procedural Geometry Workloads on Edge Architectures. In Proceedings of the 17th International Conference on Web Information Systems and Technologies - WEBIST, pages 354-359. October 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlessio Arleo, Christos Tsigkanos, Chao Jia, Roger Leite, Ilir Murturi, Manfred Klaffenböck, Schahram Dustdar, Silvia Miksch, Michael Wimmer, Johannes Sorger. Sabrina: Modeling and Visualization of Economy Data with Incremental Domain Knowledge. In IEEE VIS 2019. October 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/about/","section":"","tags":null,"title":"About Me"},{"body":"","link":"https://chaojia.github.io/tags/math/","section":"tags","tags":null,"title":"math"},{"body":"For visibility buffer rendering 1 2, we cannot rely on the hardware rasterization and built-in functions in fragment shader to do the vertex attribute interpolation and implicitly compute partial derivatives (i.e. dFdx and dFdy in glsl). This article describes the math behind a simple way to analytically compute these values, and provides formulas that can be easily translated to shader code 3. Although there are some existing implementations, they are not quite satisfactory, which motivates me to do my own math.\nIn general, we want to interpolate attributes (e.g. uv or normal) in a space before the perspective transform (object space, world space or camera space), as this is where these attributes are usually created. Therefore, in order to calculate the attributes of a pixel, we need to find the barycentric coordinates of the point in a pre-perspective space that is projected to that pixel. Because transform between pre-perspective spaces are affine transformations which preserve barycentric coordinates, without loss of generality, we can assume object space, world space and camera space are identical, and simply start with camera space.\nNotations First, some notations and conventions used throughout this article:\nindex $i \\in \\lbrace 0, 1, 2 \\rbrace$; $\\textcolor{darkgoldenrod}{i \\oplus k = (i + k) \\mod 3}$; (a bit unconventional) $X^e, X^c, X^n$ represent $X$ in camera (or eye) space, clip space and screen space (with NDC) respectively; NDC refers to normalized device coordinates in screen space (different coordinates). Attribute interpolation Suppose there is a triangle $T^e$ (in camera space) with three vertices $V^e_i = (x^e_i, y^e_i, z^e_i, 1)$. After perspective projection by Matrix $\\bm{P}$, $V^e_i$ is transformed to $V^c_i = (x^c_i, y^c_i, z^c_i, w_i) = \\bm{P}\\cdot V^e_i$ in clip space, and by applying perspective divide to $V^c_i$, we have the NDC $V^n_i = (x^n_i, y^n_i, z^n_i, 1) = V^c_i/w_i$ in screen space.\nFor a point $V^e = (x^e, y^e, z^e, 1)$ in triangle $T^e$ with the barycentric coordinates $\\lambda^e_i$, namely $V^e = \\sum \\lambda^e_i V^e_i$, its coordinates in clip space: $V^c = (x^c, y^c, z^c, w) = \\bm{P}\\cdot V^e$, and NDC: $V^n = (x^n, y^n, z^n, 1) = V^c/w$. Assume the barycentric coordinates of $V^n$ in screen space are $\\lambda^n_i$, that is, $V^n = \\sum\\lambda^n_iV^n_i$. Then we have $$ \\begin{align*} V^n \u0026amp;= \\sum\\textcolor{orangered}{\\lambda^n_i}V^n_i \\\\ \u0026amp;= \\frac{V^c}{w} = \\frac{\\bm{P}\\cdot V^e}{w} = \\frac{\\bm{P}\\sum \\lambda^e_iV^e_i}{w} \\\\ \u0026amp;= \\sum\\frac{\\lambda^e_i}{w}(\\bm{P}\\cdot V^e_i) = \\sum \\frac{\\lambda^e_i}{w}V^c_i \\\\ \u0026amp;= \\sum\\textcolor{orangered}{\\frac{\\lambda^e_i}{w}w_i}V^n_i \\end{align*} $$\nSince the barycentric coordinates of $V^n$ are unique, we have $$ \\begin{equation} \\lambda^e_i = \\frac{w\\lambda^n_i}{w_i}. \\end{equation} $$ For a given pixel location, we can lookup the corresponding triangle data based on the triangle index stored in visibility buffer. It is straightforward to compute $w_i$ and $\\lambda^n_i$ once the vertex positions are known alongside the transformation matrices and the resolution of the framebuffer. $\\lambda^n_i$ can be computed as the ratio of the areas of 2D triangles (only $x$ and $y$ components of NDC are considered), which in turn are the determinants of the matrices formed by the 2D edge vectors of these 2D triangles:\n$$ \\begin{equation} \\lambda^n_i = \\frac{1}{\\mathcal{D}}\\begin{vmatrix} x^n_{i \\oplus 1} - x^n \u0026amp; x^n_{i \\oplus 2} - x^n \\\\ y^n_{i \\oplus 1} - y^n \u0026amp; y^n_{i \\oplus 2} - y^n \\end{vmatrix} \\end{equation} $$\nwhere $\\mathcal{D}$ is twice the area of the 2D triangle $\\triangle V^n_0 V^n_1 V^n_2$, and can be calculated as\n$$ \\begin{equation*} \\mathcal{D} = \\begin{vmatrix} x^n_1 - x^n_0 \u0026amp; x^n_2 - x^n_0 \\\\ y^n_1 - y^n_0 \u0026amp; y^n_2 - y^n_0 \\end{vmatrix} \\end{equation*} $$\nDue to the fact that $1/w$ is linear in screen space (see a short proof below), we have $$ \\begin{equation} \\frac{1}{w} = \\sum\\displaystyle\\lambda^n_i\\frac{1}{w_i}. \\end{equation} $$\nBy now we have all the ingredients needed to compute $\\lambda^e_i$. It is worth noting that generally $\\lambda^e_i \\neq \\lambda^n_i$, hence the necessity for perspective correction. Let $A_i$ be an attribute of vertex $V^e_i$, for point $V^e$, the interpolated attribute is $A = \\sum\\lambda^e_i A_i$, thus $$ \\frac{A}{w} = \\sum\\frac{\\lambda^e_i}{w}A_i \\underbrace{=}_{\\text{Eq. (1)}} \\sum\\frac{\\lambda^n_i}{w_i}A_i. $$\nDerivatives Clearly $A_i$ and $w_i$ are constant, so they don't vary along $x^n$-axis or $y^n$-axis, therefore we have\n$$ \\begin{equation} \\frac{\\partial(A/w)}{\\partial x^n} = \\sum\\frac{A_i}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n}. \\end{equation} $$\nSo the partial derivative\n$$ \\begin{align*} \\frac{\\partial A}{\\partial x^n} \u0026amp;= \\frac{\\partial (w \\frac{A}{w})}{\\partial x^n} \\\\ \u0026amp;= \\frac{\\partial(\\frac{1}{w})^{-1}}{\\partial x^n}\\frac{A}{w} + w\\frac{\\partial\\frac{A}{w}}{\\partial x^n} \\\\ \u0026amp;= -\\sum\\frac{wA}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n} + \\sum\\frac{wA_i}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n}\\hspace{2em}\\Longleftarrow \\textit{Eq. (3) and Eq. (4)} \\\\ \u0026amp;= w\\sum\\left(\\frac{A_i - A}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n}\\right) \\\\ \u0026amp;= \\begin{bmatrix} \\cdots \u0026amp; A_i - A \u0026amp; \\cdots \\end{bmatrix} \\underbrace{\\begin{bmatrix} \\vdots \\\\ \\frac{w}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n} \\\\ \\vdots \\end{bmatrix}}_{\\bm{T}_x}. \\end{align*} $$\nAnalogously, the partial derivative\n$$ \\frac{\\partial A}{\\partial y^n} = \\begin{bmatrix} \\cdots \u0026amp; A_i - A \u0026amp; \\cdots \\end{bmatrix} \\underbrace{\\begin{bmatrix} \\vdots \\\\ \\frac{w}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial y^n} \\\\ \\vdots \\end{bmatrix}}_{\\bm{T}_y}. $$\nGiven Eq. (2), the last unknown pieces in $\\bm{T}_x$ and $\\bm{T}_y$ can be computed as\n$$ \\frac{\\partial \\lambda^n_i}{\\partial x^n} = \\frac{y^n_{i \\oplus 1} - y^n_{i \\oplus 2}}{\\mathcal{D}}, $$ $$ \\frac{\\partial \\lambda^n_i}{\\partial y^n} = \\frac{x^n_{i \\oplus 2} - x^n_{i \\oplus 1}}{\\mathcal{D}}. $$\nAn advantage of this approach is that once the matrices $\\bm{T_x}$ and $\\bm{T_y}$ in the above equation is calculated, it can be used to compute screen-space partial derivatives of any attributes. .\nResult The image on the left is rendered with analytically calculated attributes and derivatives, which looks identical to the render using hardware-generated attributes and derivatives on the right. The subtle difference can be revealed by some statistics of per pixel difference (normalized):\nAverage Median Maximum 0.000050 0.000000 0.054902 The color in the image above shows the difference between analytically computed and hardware-generated derivatives of uv coordinates. The difference is calculated as $\\Vert\\frac{\\partial(UV)}{\\partial x^n} - \\texttt{dFdx(UV)}\\Vert + \\Vert\\frac{\\partial(UV)}{\\partial y^n} - \\texttt{dFdy(UV)}\\Vert$ , with hardware-generated derivatives in $\\texttt{monospaced font}$. Other than a few sparse bright spots, the image is mostly black, meaning that the difference is mostly negligible, and the analytical derivatives are indeed very accurate and precise.\nPitfalls of some existing implementations The accompanying code of the original paper on visibility buffer rendering 1 computed screen-space derivatives of uv coordinates by also computing the uv coordinates of the upper and right neighboring pixels. It tends to get more computationally intensive if we have more attributes apart from uv coordinates, such as world positions and normals.\nThe DAIS paper 4 only provided Eq. (4) $\\frac{\\partial (\\lambda^n / w)}{\\partial x^n}$ regarding attribute derivatives in its appendix, and the shader code for computing derivatives given in its extended version 5 does not seem to take $\\frac{\\partial w}{\\partial x^n}$ into consideration either. In my experiment, the texture LOD inferred from derivatives calculated this way is incorrect, especially for large triangles in screen space, the render becomes blurry.\nIn rendering framework The Forge 6, the derivatives are calculated in a similar fashion to the DAIS paper, but with some obscure tweaks added which do not seem mathematically sound to me.\nProof of $1/w$ being linear in screen space Same as above, $\\bm{P}$ represents the perspective projection matrix 7 8, and we have $$\\bm{P}\\cdot V^e = \\begin{bmatrix} \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \\\\ \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \\\\ 0 \u0026amp; 0 \u0026amp; \\alpha \u0026amp; \\beta \\\\ 0 \u0026amp; 0 \u0026amp; \\gamma \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} x^e \\\\ y^e \\\\ z^e \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x^c \\\\ y^c \\\\ z^c \\\\ w \\end{bmatrix} = w \\begin{bmatrix} x^n \\\\ y^n \\\\ z^n \\\\ 1 \\end{bmatrix} . $$ After some algebraic manipulation we have $$ z^n = \\frac{z^c}{w} = \\frac{\\alpha z^e + \\beta}{w} = \\frac{\\alpha (w/\\gamma) + \\beta}{w} = \\frac{\\alpha}{\\gamma} + \\beta\\textcolor{orangered}{\\frac{1}{w}}. $$ On the other hand, $$ \\begin{aligned} z^n \u0026amp;= \\sum\\lambda^n_i z^n_i \\\\ \u0026amp;= \\sum\\lambda^n_i \\left(\\frac{\\alpha}{\\gamma} + \\beta\\frac{1}{w_i}\\right) \\\\ \u0026amp;= (\\frac{\\alpha}{\\gamma}\\sum\\lambda^n_i) + \\beta\\sum\\lambda^n_i\\frac{1}{w_i} \\\\ \u0026amp;= \\frac{\\alpha}{\\gamma} + \\beta\\textcolor{orangered}{\\sum\\lambda^n_i\\frac{1}{w_i}}.\\hspace{2em}\\Longleftarrow \\sum\\lambda^n_i = 1 \\end{aligned} $$ Therefore $$ \\frac{1}{w} = \\sum\\lambda^n_i\\frac{1}{w_i}, $$ which implies that $\\frac{1}{w}$ is linear in screen space.\nChristopher A. Burns and Warren A. Hunt, The Visibility Buffer: A Cache-Friendly Approach to Deferred Shading, Journal of Computer Graphics Techniques (JCGT), vol. 2, no. 2, 55-69, 2013\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVisibility Buffer Rendering with Material Graphs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMy glsl implementation that roughly followed this article can be found here; the derivation and the glsl implementation was already done by 29th Nov 2021, but I only had the time a few months later to get the write-up done\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChristoph Schied and Carsten Dachsbacher. 2015. Deferred attribute interpolation for memory-efficient deferred shading. In Proceedings of the 7th Conference on High-Performance Graphics (HPG '15). Association for Computing Machinery, New York, NY, USA, 43-49. https://doi.org/10.1145/2790060.2790066\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPart II Chapter 3, Listing 3.2 of the book GPU Pro 7: Advanced Rendering Techniques (1st ed.) by Engel, W. (Ed.). (2016). A K Peters/CRC Press. https://doi.org/10.1201/b21261\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAttribute derivatives in rendering framework The Forge\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReal Time Rendering, 4th Edition: $\\S$ 4.7.2 Perspective Projection\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe perspective projection matrix in Vulkan\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/posts/21-11-29-vertex-attrib-interp/","section":"posts","tags":["real-time rendering","math"],"title":"Vertex Attribute Interpolation and Analytical Derivatives"},{"body":"","link":"https://chaojia.github.io/series/","section":"series","tags":null,"title":"Series"}]