[{"body":"","link":"https://chaojia.github.io/","section":"","tags":null,"title":""},{"body":"As part of the effort to get myself familiar with Vulkan, I developed a real-time renderer featuring global illumination with RTX technique 1. Aside from Dynamic Diffuse Global Illumination (DDGI) 2 3, ray-traced soft shadow and specular reflections with spatial temporal-denoising (SVGF 4), I've also tried out some other interesting ideas.\nVisibility Buffer Rendering Visibility buffer rendering 5 6 7 has gained its popularity as triangle meshes used in real-time applications are getting much finer details, thanks to rapidly increasing memory capacity and computational power of the GPU.\nto be continued...\nSource code on gitlab\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZander Majercik, Adam Marrs, Josef Spjut, and Morgan McGuire, Scaling Probe-Based Real-Time Dynamic Global Illumination for Production, Journal of Computer Graphics Techniques (JCGT), vol. 10, no. 2, 1-29, 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZander Majercik, Jean-Philippe Guertin, Derek Nowrouzezahrai, and Morgan McGuire, Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields, Journal of Computer Graphics Techniques (JCGT), vol. 8, no. 2, 1-30, 2019\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchied, Christoph, et al. Spatiotemporal variance-guided filtering: real-time reconstruction for path-traced global illumination. Proceedings of High Performance Graphics. 2017. 1-12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChristopher A. Burns and Warren A. Hunt, The Visibility Buffer: A Cache-Friendly Approach to Deferred Shading, Journal of Computer Graphics Techniques (JCGT), vol. 2, no. 2, 55-69, 2013\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVisibility Buffer Rendering with Material Graphs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNanite - A Deep Dive\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/posts/22-08-22-vulkan-basic-renderer/","section":"posts","tags":["real-time rendering","vulkan","RTX"],"title":"A Basic Vulkan Renderer"},{"body":"","link":"https://chaojia.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://chaojia.github.io/categories/graphics/","section":"categories","tags":null,"title":"graphics"},{"body":"","link":"https://chaojia.github.io/posts/","section":"posts","tags":null,"title":"Posts"},{"body":"","link":"https://chaojia.github.io/tags/real-time-rendering/","section":"tags","tags":null,"title":"real-time rendering"},{"body":"","link":"https://chaojia.github.io/categories/rendering/","section":"categories","tags":null,"title":"rendering"},{"body":"","link":"https://chaojia.github.io/tags/rtx/","section":"tags","tags":null,"title":"RTX"},{"body":"","link":"https://chaojia.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://chaojia.github.io/tags/vulkan/","section":"tags","tags":null,"title":"vulkan"},{"body":"Real-Time C++ Vulkan Renderer with RTX global illumination Showcase of my Vulkan renderer featuring Dynamic Diffuse Global Illumination (DDGI) 1 2, ray-traced soft shadow and specular reflections with spatial-temporal denoising (SVGF 3). The renderer is implemented with C++, Vulkan and GLSL.\nSource code: https://gitlab.com/chao-jia/spock\nReal-Time Unified Physics Simulation of Variable Sized Particles This video showcases my master project done in 2017. This project is based on the position-based unified dynamic framework presented by Macklin et al 4. In this project, we removed the restriction of fixed radius of the particles in the same scene entailed by the aforementioned unified framework to reduce memory footprint of the physics simulation while maintaining the real-time performance (60+ fps on RTX 1060 Mobile).\nI also implemented and optimized algorithms for solid voxelization and construction of signed distance field on the GPU using CUDA to accelerate scene initialization. The scenes are rendered with OpenGL 4. For efficient collision detection between differently sized particles and generation of density constraints for fluids, I implemented BVH (Bounding Volume Hierarchy) construction and traversal on the GPU.\nsource code: https://gitlab.com/chao-jia/pbd\nHeightfield Water Simulation and Rendering Heightfield-based real-time water simulation with C++, Qt 5 and OpenGL 4 as a freely chosen topic for the final assignment of a practicum course in 2016 summer semester. Inspired by WebGL Water (https://madebyevan.com/webgl-water/).\nSource code: https://gitlab.com/chao-jia/height_field_water\nOther projects See here for more projects I've worked on.\nZander Majercik, Adam Marrs, Josef Spjut, and Morgan McGuire, Scaling Probe-Based Real-Time Dynamic Global Illumination for Production, Journal of Computer Graphics Techniques (JCGT), vol. 10, no. 2, 1-29, 2021\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZander Majercik, Jean-Philippe Guertin, Derek Nowrouzezahrai, and Morgan McGuire, Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields, Journal of Computer Graphics Techniques (JCGT), vol. 8, no. 2, 1-30, 2019\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchied, Christoph, et al. \u0026quot;Spatiotemporal variance-guided filtering: real-time reconstruction for path-traced global illumination.\u0026quot; Proceedings of High Performance Graphics. 2017. 1-12.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMiles Macklin, Matthias Müller, Nuttapong Chentanez, and Tae-Yong Kim. 2014. Unified particle physics for real-time applications. ACM Trans. Graph. 33, 4, Article 153 (July 2014), 12 pages.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/portfolio/","section":"","tags":null,"title":"Portfolio"},{"body":"Hi, my name is Chao Jia (IPA: /tʃaʊ. dʒʌ/). I started my work at TU Wien in 2018 as a research assistant in the Rendering and Modeling group after I got my Master's degree in computer science from Karlsruhe Institute of Technology. I'm interested in real-time rendering (OpenGL 4 and Vulkan), high-performance C++, physically based simulation and general-purpose computing on GPU (mainly CUDA, a bit OpenCL). CMake is no stranger to me either, as I've been using it as the build system for my C++ projects and vcpkg as package manager. Although I haven't worked on large python projects, I've been writing python scripts to automate many tasks.\nProjects Some of the projects I've worked on can be found in my portfolio. Here are the projects during my work at TU Wien:\nFor the paper View-Dependent Impostors for Architectural Shape Grammars 1, I extended a C++ template library for shape grammar evaluation and implemented level-of-detail mechanism for procedural geometry shape grammars; For the paper On Provisioning Procedural Geometry Workloads on Edge Architectures 2, I implemented an efficient multi-threaded web service for procedural geometry workloads using Boost Beast library in C++. Protocol buffers (Protobuf) was used for client-server communication. I have put a lot of efforts into making sure the program is platform-agnostic and made a docker container for the application to facilitate fast deployment on different edge devices; I have also implemented a GPU shape grammar evaluation system using CUDA targeting NVidia Jetson devices; For the paper Sabrina: Modeling and Visualization of Economy Data with Incremental Domain Knowledge 3, in collaboration with other research groups, I familiarized myself with Javascript, ReactJS and Postgres, and put these knowledge together to build a web application for the visualization of Austria financial data. I've also written some Python scripts for data processing (Source code on gitlab). During my free time, I have contributed a few new ports and fixes to vcpkg as I was trying to get a deeper understanding of CMake and vcpkg. For another project, I've developed a blender addon to visualize .ply format files containing points with lot of custom attributes using python (Source code on gitlab).\nPublications Chao Jia, Moritz Roth, Bernhard Kerbl, Michael Wimmer. View-Dependent Impostors for Architectural Shape Grammars. In Pacific Graphics Short Papers, Posters, and Work-in-Progress Papers, pages 63-64. October 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIlir Murturi, Chao Jia, Bernhard Kerbl, Michael Wimmer, Schahram Dustdar, Christos Tsigkanos. On Provisioning Procedural Geometry Workloads on Edge Architectures. In Proceedings of the 17th International Conference on Web Information Systems and Technologies - WEBIST, pages 354-359. October 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlessio Arleo, Christos Tsigkanos, Chao Jia, Roger Leite, Ilir Murturi, Manfred Klaffenböck, Schahram Dustdar, Silvia Miksch, Michael Wimmer, Johannes Sorger. Sabrina: Modeling and Visualization of Economy Data with Incremental Domain Knowledge. In IEEE VIS 2019. October 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/about/","section":"","tags":null,"title":"About Me"},{"body":"","link":"https://chaojia.github.io/tags/math/","section":"tags","tags":null,"title":"math"},{"body":"For visibility buffer rendering 1 2, we cannot rely on the hardware rasterization and built-in functions in fragment shader to do the vertex attribute interpolation and implicitly compute partial derivatives (i.e. dFdx and dFdy in glsl). This article describes the math behind a simple way to analytically compute these values, and provides formulas that can be easily translated to shader code. Although there are some existing implementations, they are not quite satisfactory, which motivates me to do my own math 3.\nIn general, we want to interpolate attributes (e.g. uv or normal) in a space before the perspective transform (object space, world space or camera space), as this is where these attributes are usually created. Therefore, in order to calculate the attributes of a pixel, we need to find the barycentric coordinates of the point in a pre-perspective space that is projected to that pixel. Because transform between pre-perspective spaces are affine transformations which preserve barycentric coordinates, without loss of generality, we can assume object space, world space and camera space are identical, and simply start with camera space.\nNotations First, some notations and conventions used throughout this article:\nindex $i \\in \\lbrace 0, 1, 2 \\rbrace$; $\\textcolor{darkgoldenrod}{i \\oplus k = (i + k) \\mod 3}$; (a bit unconventional) $X^e, X^c, X^n$ represent $X$ in camera (or eye) space, clip space and screen space (with NDC) respectively; NDC refers to normalized device coordinates in screen space (different coordinates). Attribute interpolation Suppose there is a triangle $T^e$ (in camera space) with three vertices $V^e_i = (x^e_i, y^e_i, z^e_i, 1)$. After perspective projection by Matrix $\\bm{P}$, $V^e_i$ is transformed to $V^c_i = (x^c_i, y^c_i, z^c_i, w_i) = \\bm{P}\\cdot V^e_i$ in clip space, and by applying perspective divide to $V^c_i$, we have the NDC $V^n_i = (x^n_i, y^n_i, z^n_i, 1) = V^c_i/w_i$ in screen space.\nFor a point $V^e = (x^e, y^e, z^e, 1)$ in triangle $T^e$ with the barycentric coordinates $\\lambda^e_i$, namely $V^e = \\sum \\lambda^e_i V^e_i$, its coordinates in clip space: $V^c = (x^c, y^c, z^c, w) = \\bm{P}\\cdot V^e$, and NDC: $V^n = (x^n, y^n, z^n, 1) = V^c/w$. Assume the barycentric coordinates of $V^n$ in screen space are $\\lambda^n_i$, that is, $V^n = \\sum\\lambda^n_iV^n_i$. Then we have $$ \\begin{align*} V^n \u0026amp;= \\sum\\textcolor{orangered}{\\lambda^n_i}V^n_i \\\\ \u0026amp;= \\frac{V^c}{w} = \\frac{\\bm{P}\\cdot V^e}{w} = \\frac{\\bm{P}\\sum \\lambda^e_iV^e_i}{w} \\\\ \u0026amp;= \\sum\\frac{\\lambda^e_i}{w}(\\bm{P}\\cdot V^e_i) = \\sum \\frac{\\lambda^e_i}{w}V^c_i \\\\ \u0026amp;= \\sum\\textcolor{orangered}{\\frac{\\lambda^e_i}{w}w_i}V^n_i \\end{align*} $$\nSince the barycentric coordinates of $V^n$ are unique, we have $$ \\begin{equation} \\lambda^e_i = \\frac{w\\lambda^n_i}{w_i}. \\end{equation} $$ For a given pixel location, we can lookup the corresponding triangle data based on the triangle index stored in visibility buffer. It is straightforward to compute $w_i$ and $\\lambda^n_i$ once the vertex positions are known alongside the transformation matrices and the resolution of the framebuffer. $\\lambda^n_i$ can be computed as the ratio of the areas of 2D triangles (only $x$ and $y$ components of NDC are considered), which in turn are the determinants of the matrices formed by the 2D edge vectors of these 2D triangles:\n$$ \\begin{equation} \\lambda^n_i = \\frac{1}{\\mathcal{D}}\\begin{vmatrix} x^n_{i \\oplus 1} - x^n \u0026amp; x^n_{i \\oplus 2} - x^n \\\\ y^n_{i \\oplus 1} - y^n \u0026amp; y^n_{i \\oplus 2} - y^n \\end{vmatrix} \\end{equation} $$\nwhere $\\mathcal{D}$ is twice the area of the 2D triangle $\\triangle V^n_0 V^n_1 V^n_2$, and can be calculated as\n$$ \\begin{equation*} \\mathcal{D} = \\begin{vmatrix} x^n_1 - x^n_0 \u0026amp; x^n_2 - x^n_0 \\\\ y^n_1 - y^n_0 \u0026amp; y^n_2 - y^n_0 \\end{vmatrix} \\end{equation*} $$\nDue to the fact that $1/w$ is linear in screen space (see a short proof below), we have $$ \\begin{equation} \\frac{1}{w} = \\sum\\displaystyle\\lambda^n_i\\frac{1}{w_i}. \\end{equation} $$\nBy now we have all the ingredients needed to compute $\\lambda^e_i$. It is worth noting that generally $\\lambda^e_i \\neq \\lambda^n_i$, hence the necessity for perspective correction. Let $A_i$ be an attribute of vertex $V^e_i$, for point $V^e$, the interpolated attribute is $A = \\sum\\lambda^e_i A_i$, thus $$ \\frac{A}{w} = \\sum\\frac{\\lambda^e_i}{w}A_i \\underbrace{=}_{\\text{Eq. (1)}} \\sum\\frac{\\lambda^n_i}{w_i}A_i. $$\nDerivatives Clearly $A_i$ and $w_i$ are constant, so they don't vary along $x^n$-axis or $y^n$-axis, therefore we have\n$$ \\begin{equation} \\frac{\\partial(A/w)}{\\partial x^n} = \\sum\\frac{A_i}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n}. \\end{equation} $$\nSo the partial derivative\n$$ \\begin{align*} \\frac{\\partial A}{\\partial x^n} \u0026amp;= \\frac{\\partial (w \\frac{A}{w})}{\\partial x^n} \\\\ \u0026amp;= \\frac{\\partial(\\frac{1}{w})^{-1}}{\\partial x^n}\\frac{A}{w} + w\\frac{\\partial\\frac{A}{w}}{\\partial x^n} \\\\ \u0026amp;= -\\sum\\frac{wA}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n} + \\sum\\frac{wA_i}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n}\\hspace{2em}\\Longleftarrow \\textit{Eq. (3) and Eq. (4)} \\\\ \u0026amp;= w\\sum\\left(\\frac{A_i - A}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n}\\right) \\\\ \u0026amp;= \\begin{bmatrix} \\cdots \u0026amp; A_i - A \u0026amp; \\cdots \\end{bmatrix} \\underbrace{\\begin{bmatrix} \\vdots \\\\ \\frac{w}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial x^n} \\\\ \\vdots \\end{bmatrix}}_{\\bm{T}_x}. \\end{align*} $$\nAnalogously, the partial derivative\n$$ \\frac{\\partial A}{\\partial y^n} = \\begin{bmatrix} \\cdots \u0026amp; A_i - A \u0026amp; \\cdots \\end{bmatrix} \\underbrace{\\begin{bmatrix} \\vdots \\\\ \\frac{w}{w_i}\\frac{\\partial \\lambda^n_i}{\\partial y^n} \\\\ \\vdots \\end{bmatrix}}_{\\bm{T}_y}. $$\nGiven Eq. (2), the last unknown pieces in $\\bm{T}_x$ and $\\bm{T}_y$ can be computed as\n$$ \\frac{\\partial \\lambda^n_i}{\\partial x^n} = \\frac{y^n_{i \\oplus 1} - y^n_{i \\oplus 2}}{\\mathcal{D}}, $$ $$ \\frac{\\partial \\lambda^n_i}{\\partial y^n} = \\frac{x^n_{i \\oplus 2} - x^n_{i \\oplus 1}}{\\mathcal{D}}. $$\nAn advantage of this approach is that once the matrices $\\bm{T_x}$ and $\\bm{T_y}$ in the above equation is calculated, it can be used to compute screen-space partial derivatives of any attributes.\nResult The image on the left is rendered with analytically calculated attributes and derivatives, which looks identical to the render using hardware-generated attributes and derivatives on the right. The subtle difference can be revealed by some statistics of per pixel difference (normalized):\nAverage Median Maximum 0.000050 0.000000 0.054902 The color in the image above shows the difference between analytically computed and hardware-generated derivatives of uv coordinates. The difference is calculated as $\\Vert\\frac{\\partial(UV)}{\\partial x^n} - \\texttt{dFdx(UV)}\\Vert + \\Vert\\frac{\\partial(UV)}{\\partial y^n} - \\texttt{dFdy(UV)}\\Vert$ , with hardware-generated derivatives in $\\texttt{monospaced font}$. Other than a few sparse bright spots, the image is mostly black, meaning that the difference is mostly negligible, and the analytical derivatives are indeed very accurate and precise.\nPitfalls of some existing implementations The accompanying code of the original paper on visibility buffer rendering 1 computed screen-space derivatives of uv coordinates by also computing the uv coordinates of the upper and right neighboring pixels. It tends to get more computationally intensive if we have more attributes apart from uv coordinates, such as world positions and normals.\nThe DAIS paper 4 only provided Eq. (4) $\\frac{\\partial (\\lambda^n / w)}{\\partial x^n}$ regarding attribute derivatives in its appendix, and the shader code for computing derivatives given in its extended version 5 does not seem to take $\\frac{\\partial w}{\\partial x^n}$ into consideration either. In my experiment, the texture LOD inferred from derivatives calculated this way is incorrect, especially for large triangles in screen space, the render becomes blurry.\nIn rendering framework The Forge 6, the derivatives are calculated in a similar fashion to the DAIS paper, but with some obscure tweaks added which do not seem mathematically sound to me.\nProof of $1/w$ being linear in screen space Same as above, $\\bm{P}$ represents the perspective projection matrix 7 8, and we have $$\\bm{P}\\cdot V^e = \\begin{bmatrix} \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \\\\ \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \u0026amp; \\cdot \\\\ 0 \u0026amp; 0 \u0026amp; \\alpha \u0026amp; \\beta \\\\ 0 \u0026amp; 0 \u0026amp; \\gamma \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} x^e \\\\ y^e \\\\ z^e \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x^c \\\\ y^c \\\\ z^c \\\\ w \\end{bmatrix} = w \\begin{bmatrix} x^n \\\\ y^n \\\\ z^n \\\\ 1 \\end{bmatrix} . $$ After some algebraic manipulation we have $$ z^n = \\frac{z^c}{w} = \\frac{\\alpha z^e + \\beta}{w} = \\frac{\\alpha (w/\\gamma) + \\beta}{w} = \\frac{\\alpha}{\\gamma} + \\beta\\textcolor{orangered}{\\frac{1}{w}}. $$ On the other hand, $$ \\begin{aligned} z^n \u0026amp;= \\sum\\lambda^n_i z^n_i \\\\ \u0026amp;= \\sum\\lambda^n_i \\left(\\frac{\\alpha}{\\gamma} + \\beta\\frac{1}{w_i}\\right) \\\\ \u0026amp;= (\\frac{\\alpha}{\\gamma}\\sum\\lambda^n_i) + \\beta\\sum\\lambda^n_i\\frac{1}{w_i} \\\\ \u0026amp;= \\frac{\\alpha}{\\gamma} + \\beta\\textcolor{orangered}{\\sum\\lambda^n_i\\frac{1}{w_i}}.\\hspace{2em}\\Longleftarrow \\sum\\lambda^n_i = 1 \\end{aligned} $$ Therefore $$ \\frac{1}{w} = \\sum\\lambda^n_i\\frac{1}{w_i}, $$ which implies that $\\frac{1}{w}$ is linear in screen space.\nChristopher A. Burns and Warren A. Hunt, The Visibility Buffer: A Cache-Friendly Approach to Deferred Shading, Journal of Computer Graphics Techniques (JCGT), vol. 2, no. 2, 55-69, 2013\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVisibility Buffer Rendering with Material Graphs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe calculation was done by 29th Nov 2021, but I only had the time a few months later to get the write-up done.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nChristoph Schied and Carsten Dachsbacher. 2015. Deferred attribute interpolation for memory-efficient deferred shading. In Proceedings of the 7th Conference on High-Performance Graphics (HPG '15). Association for Computing Machinery, New York, NY, USA, 43–49. https://doi.org/10.1145/2790060.2790066\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPart II Chapter 3, Listing 3.2 of the book GPU Pro 7: Advanced Rendering Techniques (1st ed.) by Engel, W. (Ed.). (2016). A K Peters/CRC Press. https://doi.org/10.1201/b21261\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAttribute derivatives in rendering framework The Forge\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReal Time Rendering, 4th Edition: $\\S$ 4.7.2 Perspective Projection\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe perspective projection matrix in Vulkan\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"https://chaojia.github.io/posts/21-11-29-vertex-attrib-interp/","section":"posts","tags":["real-time rendering","math"],"title":"Vertex Attribute Interpolation and Analytical Derivatives"},{"body":"","link":"https://chaojia.github.io/series/","section":"series","tags":null,"title":"Series"}]